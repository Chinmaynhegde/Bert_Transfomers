{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3Eztk5OcniH"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "unmasker(\"Hello I'm a [MASK] model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n"
      ],
      "metadata": {
        "id": "FLfcao_lcsuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlQ5JocHdAZ7",
        "outputId": "ec67d0cc-3c3c-496c-d9b9-46c9225b4211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.pipelines.fill_mask.FillMaskPipeline at 0x7e5bca5b6e60>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline for masked language modeling\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "\n",
        "# Perform masked language modeling on a given text\n",
        "result = unmasker(\"Hello I'm a [MASK] model.\")\n",
        "\n",
        "# Print the results\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "id": "jYa4u3GedJwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define the input text\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Print the output\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "RY5FiPjBdooU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "unmasker(\"The man worked as a [MASK].\")\n",
        "\n",
        "\n",
        "# unmasker(\"The woman worked as a [MASK].\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJz_vtu1eHaM",
        "outputId": "96802975-6f99-4129-ead8-a0c3f0a3fa47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.09747558832168579,\n",
              "  'token': 10533,\n",
              "  'token_str': 'carpenter',\n",
              "  'sequence': 'the man worked as a carpenter.'},\n",
              " {'score': 0.05238332226872444,\n",
              "  'token': 15610,\n",
              "  'token_str': 'waiter',\n",
              "  'sequence': 'the man worked as a waiter.'},\n",
              " {'score': 0.049626998603343964,\n",
              "  'token': 13362,\n",
              "  'token_str': 'barber',\n",
              "  'sequence': 'the man worked as a barber.'},\n",
              " {'score': 0.037886131554841995,\n",
              "  'token': 15893,\n",
              "  'token_str': 'mechanic',\n",
              "  'sequence': 'the man worked as a mechanic.'},\n",
              " {'score': 0.037680815905332565,\n",
              "  'token': 18968,\n",
              "  'token_str': 'salesman',\n",
              "  'sequence': 'the man worked as a salesman.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model in spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Tokenization and lemmatization\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Example text\n",
        "example_text = \"This is an example sentence for preprocessing. It includes punctuation, such as commas and periods.\"\n",
        "\n",
        "# Preprocess the example text\n",
        "preprocessed_text = preprocess_text(example_text)\n",
        "print(preprocessed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKWSqQgCfG75",
        "outputId": "7e32f5d5-454e-4373-c8e7-01fb113055f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'preprocessing', 'include', 'punctuation', 'comma', 'period']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and symbols\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Example text\n",
        "example_text = \"This is an example sentence for preprocessing. It includes punctuation, such as commas and periods.\"\n",
        "\n",
        "# Preprocess the example text\n",
        "preprocessed_text = preprocess_text(example_text)\n",
        "print(preprocessed_text)\n"
      ],
      "metadata": {
        "id": "k8VprKZfibGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "qa_pipeline = pipeline(\"question-answering\")\n",
        "\n",
        "# Provide the context (the passage or document)\n",
        "context = \"\"\"\n",
        "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\n",
        "It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\n",
        "Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially criticized\n",
        "by some of France's leading artists and intellectuals for its design, but it has become a global\n",
        "cultural icon of France and one of the most recognisable structures in the world. The Eiffel Tower\n",
        "is the most-visited paid monument in the world; 6.91 million people ascended it in 2015.\n",
        "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building,\n",
        "and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side.\n",
        "During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest\n",
        "man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York\n",
        "City was finished in 1930. It was the first structure to reach a height of 300 metres.\n",
        "Due to the addition of a broadcasting aerial at the top of the tower in 1957,\n",
        "it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters,\n",
        "the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\n",
        "\"\"\"\n",
        "\n",
        "# Ask a question related to the context\n",
        "question = \"What is the material Eiffel tower is made up of?\"\n",
        "\n",
        "# Use the pipeline to get the answer\n",
        "answer = qa_pipeline(question=question, context=context)\n",
        "\n",
        "# Print the answer\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer['answer']}\")\n"
      ],
      "metadata": {
        "id": "I7c59JXKGCf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaauwUQEkKlK",
        "outputId": "3194681e-9e31-4155-f4ad-9e55211d9ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/239.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF\n"
      ],
      "metadata": {
        "id": "PXXc2e2klOX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "import torch\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "# Load the PDF document\n",
        "pdf_path = \"chinmayYt.pdf\"\n",
        "\n",
        "# Extract text from the PDF\n",
        "text = \"\"\n",
        "with fitz.open(pdf_path) as pdf:\n",
        "    for page in pdf:\n",
        "        text += page.get_text()\n",
        "\n",
        "# Ask a question\n",
        "question = \"What is the main topic of this document?\"\n",
        "\n",
        "# Tokenize input text and question\n",
        "inputs = tokenizer(question, text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "# Get model predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract answer from the model output\n",
        "answer_start = torch.argmax(outputs.start_logits)\n",
        "answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "_I6f9ETuGaJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "import torch\n",
        "import docx\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "# Load the Word document\n",
        "doc_path = \"demo.docx\"\n",
        "doc = docx.Document(doc_path)\n",
        "\n",
        "# Initialize variables to store the combined text and answers\n",
        "combined_text = \"\"\n",
        "combined_answer = \"\"\n",
        "\n",
        "# Process each page/segment of the document\n",
        "for i, page in enumerate(doc.paragraphs):\n",
        "    # Extract text from the current page/segment\n",
        "    segment_text = page.text\n",
        "    combined_text += segment_text + \"\\n\"\n",
        "\n",
        "    # Ask a question for each segment\n",
        "    question = \"What is the main topic of this document?\"\n",
        "\n",
        "    # Tokenize input text and question\n",
        "    inputs = tokenizer(question, segment_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract answer from the model output\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
        "\n",
        "    # Combine answers from each segment\n",
        "    combined_answer += f\"Answer for segment {i+1}: {answer}\\n\"\n",
        "\n",
        "# Print the combined text and answers\n",
        "print(\"Combined Text:\\n\", combined_text)\n",
        "print(\"\\nCombined Answers:\\n\", combined_answer)\n"
      ],
      "metadata": {
        "id": "rUV2-yxghMeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets\n"
      ],
      "metadata": {
        "id": "cWaSqB52KSZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load your custom SQuAD dataset from local file\n",
        "with open(\"train.json\", \"r\") as f:\n",
        "    data = json.load(f)[\"data\"]\n",
        "\n",
        "# Extract questions, contexts, and answers from your dataset\n",
        "questions = []\n",
        "contexts = []\n",
        "answers = []\n",
        "for paragraph in data:\n",
        "    for qa in paragraph[\"paragraphs\"]:\n",
        "        for qa_pair in qa[\"qas\"]:\n",
        "            questions.append(qa_pair[\"question\"])\n",
        "            contexts.append(qa[\"context\"])\n",
        "            # Extracting the first answer, you might want to modify this if multiple answers are present\n",
        "            answers.append(qa_pair[\"answers\"][0][\"text\"])\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize questions and contexts\n",
        "tokenized_inputs = tokenizer(questions, contexts, padding='max_length', truncation=True, return_tensors='pt')\n",
        "\n",
        "# Custom dataset class\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, tokenized_inputs, answers):\n",
        "        self.tokenized_inputs = tokenized_inputs\n",
        "        self.answers = answers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.answers)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: tensor[idx] for key, tensor in self.tokenized_inputs.items()}, self.answers[idx]\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = QADataset(tokenized_inputs, answers)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(3):  # Adjust the number of epochs as needed\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        batch_inputs, batch_answers = batch\n",
        "        input_ids = batch_inputs['input_ids'].to(device)\n",
        "        attention_mask = batch_inputs['attention_mask'].to(device)\n",
        "        start_positions, end_positions = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Compute the loss\n",
        "        start_positions_loss = loss_fn(start_positions, batch_inputs['start_positions'].to(device))\n",
        "        end_positions_loss = loss_fn(end_positions, batch_inputs['end_positions'].to(device))\n",
        "        loss = (start_positions_loss + end_positions_loss) / 2\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Average Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
      ],
      "metadata": {
        "id": "c957Es3_mhsU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "8d1d3375-22c0-4d5b-eac7-74d1f49912bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'start_positions'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-81cf429c0142>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mstart_positions_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_positions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mend_positions_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end_positions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_positions_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend_positions_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'start_positions'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nxIrT70pRH7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}